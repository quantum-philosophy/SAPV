Let us turn to Stage~3 in \fref{algorithm}.
We have at our disposal $\hat{\vparam}$ and $\mOI$ from Stage~2 in order to construct an adequate proposal and utilize it for sampling.
A commonly used proposal is a multivariate Gaussian distribution wherein the mean is the current location of the chain of samples started at $\hat{\vparam}$, and the covariance matrix is the inverse of $\mOI$ \cite{gelman2004, bernardo2007}.
In order to speed up the sampling process, we would like to make use of the potential of multicore parallelization.
The above proposal, however, is purely sequential as the mean for the next sample draw is dependent on the previous sample.
Therefore, we appeal to a variation of the MH algorithm known as the independence sampler \cite{gelman2004}.
In this case, a typical choice of the proposal is a multivariate t-distribution, independent of the current position of the chain:
\begin{equation} \elabel{proposal}
  \vparam \sim \studentst{\nu}{\hat{\vparam}}{\alpha^2 \mOI^{-1}}
\end{equation}
where $\hat{\vparam}$ and $\mOI$ are as in \sref{optimization}, $\nu$ is the number of degrees of freedom, and $\alpha$ is a tuning constant controlling the standard deviation of the proposal.
Now the proposal samples and the time-consuming evaluation of their posterior in \eref{posterior} can be computed for all samples in parallel.
Then the precomputed samples can subsequently be accepted or rejected as in the usual MH algorithm.

Having completed the sampling procedure, we obtain a collection of samples of $\vparam$. The first portion of the drawn samples is typically discarded before the final computations as being unrepresentative; this portion is also known as the burn-in period.
Each of the preserved samples of $\vparam$, comprising $\vz$, $\mu_\u$, and $\sigma^2_\u$, is then used in \eref{kl-approximation} to compute a sample of $\u$, $\vu_i \in \real^{\ndies \nprocs}$.
Denote such a data set with $\nsamples$ samples of the \qoi\ by $\UData = \{ \vu_i \}_{i = 1}^\nsamples$.
