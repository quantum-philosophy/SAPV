\input{include/figures/wafer-qoi.tex}
Let us consider an important application of the proposed technique: the characterization of the distribution, across a silicon wafer, of the effective channel length, denoted by $\u$. The effective channel length has one of the strongest effects on the subthreshold leakage current and, consequently, on power and temperature \cite{juan2011, juan2012}; at the same time, $\u$ is well-known to be severely deteriorated by process variation \cite{chandrakasan2001, srivastava2010}.
Assume the technological process imposes a lower bound $\u_*$ on $\u$.\footnote{For simplicity, a possible upper bound on the effective channel length is ignored in the motivational example.} This bound separates defective dies ($\u < \u_*$) from those that are acceptable ($\u \geq \u_*$).
In order to reduce costs, the manufacturer is interested in detecting the faulty dies and taking them out of the production process at early stages.
Then the possible actions that they might take with respect to a single die on the wafer are: (a) keep the die if it conforms to the specification; (b) recycle the die otherwise.
Let the distribution of $\u$ across the wafer be the one depicted on the left side of \fref{wafer-qoi}.
The gradient from navy to dark red represents the transition of $\u$ from low to high values; hence, the navy regions have a high level of the power/heat dissipation.\footnote{The experimental setup will be described in detail in \sref{experimental-results}.}
In order to characterize the uncertainty due to the effective channel length $\u$, one can find the above-mentioned distribution by removing the top layer of (thus, destroying) the dies and measuring $\u$ directly.
Alternatively, we can restrain our ambitions and change the \qoi\ to some other parameter, which can be measured without the need of damaging the dies, \eg, to the leakage current.
In either way, one has to deploy adequate test structures on the dies and take direct measurements of the initial/surrogate \qoi. The problem, however, is that the described procedure is technologically complex and unpractical; thus, it might significantly increase the production costs.
Furthermore, the first approach implies that all the measured dies have to be thrown away afterwards, and the second implies that the design decisions will be based on a surrogate quantity instead of the primary source of uncertainty, \ie, the effective channel length, which can compromise the reliability of such decisions.

Our technique works differently. We apply a fixed workload to only a small number of dies on the wafer, measure the corresponding transient temperature profiles, and characterize the \qoi, which is the effective channel length in the current example, by analyzing the obtained data using Bayesian inference.
The collected profiles can potentially be corrupted by the measurement noise. Since temperature is cheaper to track using, \eg, infrared cameras and no on-die test structures are required \cite{mesa-martinez2007}, our approach can decrease the costs associated with the characterization of process variation and further decision making.
The result of our framework applied to a set of noisy temperature profiles measured for only 20 out of 316 dies on the wafer is shown on the right side of \fref{wafer-qoi}, and the locations of the dies selected for temperature measurements are depicted in \fref{wafer-measured}.
It can be seen that the two maps in \fref{wafer-qoi} closely match each other implying that, based on less than $7\%$ of the dies measured, our approach is able to reconstruct the distribution of the effective channel length with a high level of accuracy.

Another feature of the proposed framework is that probabilities of various events, \eg, $\probabilityMeasure(\u \geq \u_*)$, can readily be estimated.
This is important since, in reality, the true values of the \qoi\ are unknown for us (otherwise, we would not need to quantify them), and, therefore, we can rely on our decisions only up to a certain probability.
We can then reformulate the decision rule defined earlier as follows: (a) keep the die if $\probabilityMeasure(\u \geq \u_*)$ is larger than a certain threshold; (b) recycle the die otherwise.
An illustration of this rule is given in \fref{wafer-defective} where the lower bound $\u_*$ is set to two standard deviations below the mean value of the effective channel length; the probability threshold of the action (a) is set to 0.9; the crosses mark both the true and inferred defective dies (they coincide); and the gradient from light gray to red corresponds to the inferred probability of a die to be defective. It can be seen that the inference accurately detects faulty regions.
\input{include/figures/wafer-measured-defective.tex}

In addition, we can introduce a trade-off action: (c) expose the die to a thorough inspection (\eg, via a test structure) if $\probabilityMeasure(\u \geq \u_*)$ is smaller than the threshold of (a) and is larger than some other threshold, \eg, $0.1 < \probabilityMeasure(\u \geq \u_*) < 0.9$. In this case, we can reduce costs by examining only those dies for which there is neither sufficiently strong evidence of their satisfactory nor unsatisfactory condition.
Furthermore, one can take into consideration a so-called utility function, which, for each combination of an outcome of $\u$ and a taken action, returns the gain that the decision maker obtains.
For example, such a function can favor a rare omission of malfunctioning dies to a frequent inspection of correct dies as the latter might involve much more costs.
The optimal decision is given by the action that maximizes the expected utility with respect to both the observed data and prior knowledge on $\u$ \cite{bernardo2007}.
Thus, all possible $\u$ weighted by their probabilities will be taken into account in the final decision, incorporating also the preferences of the user via the utility function.
