Let $\u: \outcomes \times \domain \to \real$ be a square-integrable stochastic process defined over a domain $\domain$ \cite{durrett2010}. Let $\fMean{\r} := \E{\u(\r)}$ and $\fCov{\r, \r'} := \E{(\u(\r) - \fMean{\r}) (\u(\r') - \fMean{\r'})}$, $\r, \r' \in \domain$, be the mean and covariance functions of $\u$, respectively. Take a vector $\vr = (\r_i) \in \domain^n$, and define the corresponding discretization of $\u$ as an $n$-dimensional \rv\ $\vu = (\u(\r_i)) \in \real^n$ endowed with the expected value $\vmean = (\fMean{\r_i}) \in \real^n$ and the covariance matrix $\mCov = (\fCov{\r_i, \r_j}) \in \real^{n \times n}$. Since any $\mCov$ is real and symmetric, it admits the eigenvalue decomposition \cite{press2007}: $\mCov = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda} = \diag{\lambda_i}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov$, respectively. Denoting $\mKL = \m{V} \m{\Lambda}^\frac{1}{2}$, $\vu$ can be represented as $\vu = \vmean + \mKL \vz$ where $\vz$ is a vector of centered, normalized, and uncorrelated \rvs, which are also independent if $\u$ is Gaussian.
The decomposition above provides means for model order reduction as, due to the correlations induced by $\fCov$, $\vu$ can be recovered from a small subset of $\vz$. Such redundancies can be revealed by analyzing the eigenvalues $\lambda_i$ stored in $\m{\Lambda}$.
The goal is to identify the smallest subset of $\lambda_i$ such that its contribution to the sum of all $\lambda_i$ is grater than a certain threshold.
When this threshold is close to one, the rest of the eigenvalues and their eigenvectors can be dropped as being insignificant, reducing the number of stochastic dimensions.
