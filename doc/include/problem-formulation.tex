Consider a generic electronic system, which is fabricated on a silicon wafer hosting $\ndies$ dies.
Each die consists of $\nprocs$ active components identified at the intended level of granularity (processors, ALUs, caches, register files, \etc); hereafter, these components are referred to as processing elements.
The system depends on a process parameter $\u$, which we are interested in studying and shall refer to as the quantity of interest (\qoi).
Due to the presence of process variation, the value of $\u$ deviates from the nominal one, and this deviation can be different at different locations on the wafer.
The \qoi\ is assumed to be impractical for direct measurements.

The goal of this work is to develop a statistical framework targeted at the identification of the on-wafer distribution of $\u$ with the following properties: (a) low measurement costs; (b) high computational speed; (c) robustness to the measurement noise; (d) ability to assess the trustworthiness of the collected data and corresponding predictions; and (e) ability to accommodate prior information on $\u$.

In order to achieve the aforementioned goal, we propose the use of indirect measurements.
Specifically, instead of $\u$, we measure an auxiliary parameter $\q$, which we shall refer to as the quantity of measure (\qom).
The observations of $\q$ are then processed via Bayesian inference in order to derive the distribution of the \qoi, $\u$.
The \qom\ is chosen such that: (a) $\q$ is convenient and cheap to be tracked; (b) $\q$ depends on $\u$, which is signified by $\q = \oBB{\u}$; and (c) there is a way to compute $\q$ for a given $\u$.
The last means that $\oBB$ should be known; however, it does not have to be explicit since $\oBB$ appears inside our framework as a ``black box''.
For example, $\oBB$ can be a piece of code or an output of an adequate simulator.

As the first step, the user of the proposed framework is supposed to harvest a set of observations of $\q$ at several locations on the wafer (recall \sref{motivation}).
Although these locations can be arbitrary, for clarity of presentation, we let one die refer to one (potential) measurement site, and this site can comprise up to $\nprocs$ measurement points corresponding to the centers of the processing elements.
Denote by $\QData = \{ \q_i^\meas \}_{i = 1}^\nrdies$ such a data set composed of $\nrdies$ observations $\q_i^\meas$ of $\q$, which are made at $\nrprocs$ out of $\nprocs$ processing elements located at $\nrdies$ out of $\ndies$ dies on the wafer.
It is implied that the site of each observation is recorded as well.
The dimensionality of $\q_i^\meas$ is $\nprocs \times \nrsteps$ where $\nrsteps$ is the amount of data captured per processing element.
For example, in \sref{motivation}, $\q_i^\meas$ was an $\nprocs \times \nrsteps$ matrix capturing temperature of $\nprocs$ processing elements, located on the $i$th measured die, for $\nrsteps$ moments of time.
Note that $\QData$ typically contains data only for a small subset of the dies on the wafer, \ie, $\nrdies \ll \ndies$.

For convenience, we denote by $\specification$ all the information relevant to the production and measurement processes including: (a) the layout of the wafer and (b) the floorplan of a die on the wafer.
