Let $\vparam$ be a set of parameters that are uncertain for us either by nature or due to our limited knowledge. The goal is to characterize $\vparam$ given (a) a data set of observations $\Data$, (b) a model for $\Data$ parametrized by $\vparam$, and (c) prior believes on $\vparam$. The last can originate from our previous experience, experts, \etc\ A natural solution is to rely on Bayes' rule \cite{gelman2004}:
\begin{equation} \elabel{bayes}
  \text{\small Posterior} = \frac{\text{\small Likelihood} \times \text{\small Prior}}{\text{\small Evidence}}, \; \f{\vparam | \Data} = \frac{\f{\Data | \vparam} \f{\vparam}}{\f{\Data}},
\end{equation}
where $\f{\cdot}$ denotes a probability density function, and the vertical bars stand for conditioning. $\f{\vparam}$ is called the prior of $\vparam$, $\f{\vparam | \Data}$ is the corresponding posterior, $\f{\Data | \vparam}$ is the likelihood function, and $\f{\Data}$ is a normalization constant.
Out-of-sandbox problems often do not have closed-form expressions for posterior distributions. This is mainly due to the forward model involved in the likelihood function. For instance, a physical phenomenon, such as heat transfer, may introduce a system of differential equations in $\f{\Data | \vparam}$, which is the case for our problem (see \sref{thermal-model}).
Thus, in order to be able to draw samples from the posterior, one usually relies on Monte Carlo (MC) sampling and, in particular, on Markov Chain Monte Carlo (MCMC) sampling \cite{gelman2004}, which we also do.
