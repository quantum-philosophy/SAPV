In this section, we describe the objective of Stage~2 in \fref{algorithm}.
Although the requirements to the proposal distribution mentioned earlier are rather weak, it is often difficult to pick an efficient proposal, which would yield a good approximation with as fewer evaluations of the posterior in \eref{posterior} and, thus, of the data model in \sref{data-model} as possible.
This choice is especially severe for high-dimensional problems, and our problem, involving around 30 parameters as we shall see in \sref{experimental-results}, is one them.
Therefore, a careful construction of the proposal is an essential component of our framework.
A common technique to construct a high-quality proposal is to perform an optimization procedure of the posterior given by \eref{posterior}.
More specifically, we seek for such a value $\hat{\vparam}$ of $\vparam$ that maximizes \eref{posterior} and, hence, has the maximal posterior probability.
We also compute the negative of the Hessian matrix at $\hat{\vparam}$, which is called the observed information matrix and denoted by $\mOI$ (see the output of Stage~2 in \fref{algorithm}).
Using $\hat{\vparam}$ and $\mOI$, we can now construct such a proposal, which will allow the MH algorithm (a) to start producing samples directly from the desired regions of high probability and (b) to explore those regions more rapidly.
