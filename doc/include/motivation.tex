Let us consider an example that illustrates a particular application of the proposed technique. As previously mentioned, due to process variation, the key parameters that have direct impacts on power and temperature are intrinsically uncertain. Let $\u$ be one of such uncertain quantities. Assume the manufacturing process imposes a lower bound $\u_*$ on the parametrization $\u$. This lower bound separates defective dies ($\u_* > \u$) from those that function properly ($\u_* \leq \u$). Possible actions that one might wish to take with respect to a single die on the wafer are: (a) keep the die if it closely conforms to the specification; (b) throw away the die if it exhibits an unacceptable divergence, due to process variation, from the specification. A common approach to find the distribution of $\u$, both across the wafer and within a single die, is to deploy adequate test structures on the dies and measure $\u$ directly; then, the corresponding decision can be taken based on the collected information. The problem, however, is that the described procedure is typically expensive to undertake. On the contrary, the technique that we propose and develop in this paper operates on cheap indirect measurements, denoted by $\Data$, and, therefore, can considerably decrease these costs. Using the proposed framework to infer $\u$, we modify the list of possible actions as follows: (a) keep the die if there is no evidence in $\Data$ of its malfunction; (b) throw the die away if there is a strong indication in $\Data$ of its faulty. In addition, we can introduce a trade-off action: (c) expose the die to a thorough inspection (\eg, via a test structure) if (a) and (b) cannot be accepted with a sufficient level of confidence.
