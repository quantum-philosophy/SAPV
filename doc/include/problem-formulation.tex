Let $\probabilitySpace$ be a probability space where $\outcomes$ is a set of outcomes, $\sigmaAlgebra$ is a $\sigma$-algebra on $\outcomes$, and $\probabilityMeasure: \sigmaAlgebra \to [0, 1]$ is a probability measure \cite{durrett2010}. A random variable (\rv) is defined as a $\sigmaAlgebra$-measurable function $X: \outcomes \to \real$. An $n$-dimensional \rv\ is then a vector of $n$ \rvs. In what follows, the probability space $\probabilitySpace$ is always implied.

Consider a heterogeneous multiprocessor system that consists of $\nprocs$ processing elements and is equipped with a thermal package. The processing elements are the active components of the system identified at the intended level of granularity (processors, ALUs, caches, registers, \etc).
Let $\specification$ be a thermal specification of the system defined as a collection of temperature-related information: (a) the floorplans of the active layers of the chip; (b) the geometry of the thermal package; (c) the thermal parameters of the materials that the chip and package are made of.
In addition, the system depends on an uncertain parametrization $\u$, which manifests itself in deviations of the actual power dissipation from nominal values and, consequently, in deviations of temperature from the one corresponding to the nominal power consumption.

A power profile $\profileP$ is a tuple composed of a data matrix $\mP = (\vP_i) \in \real^{\nprocs \times \npsteps}$, $\vP_i \in \real^\nprocs$, that captures the power dissipation of all the $\nprocs$ processing elements at $\npsteps$ moments of time and a (column) vector $\partition{\mP} = (\t_i) \in \real^{\npsteps}$ with positive and strictly increasing components that specifies these moments of time.
The definition of a temperature profile $\profileT$ is the same as the one for power except that the data matrix $\mT = (\vT_i) \in \real^{\nprocs \times \nsteps}$, $\vT_i \in \real^\nprocs$, contains temperature.

The goal of this work is to develop a multiprocessor system design framework for the characterization of the uncertain parametrization $\u$ such that the framework possesses the following properties: (a) low deployment costs, (b) computational efficiency, and (c) ability to operate on incomplete and noisy data.
To this end, we propose the use of indirect measurements; specifically, we aim to measure temperature as it was described in \sref{motivation}.
The inputs to our framework are: (a) the thermal specification $\specification$ of the platform, (b) a test workload given as a dynamic power profile $\profilePdyn$, and (c) a set of temperature profiles, corresponding to $\profilePdyn$, collected at several locations on the wafer. Denote by
\[
  \Data = \left\{ (\mT^{(i)}_\meas, \partition{\mT}), \r_i \right\}_{i = 1}^\ndata
\]
such a data set composed of $\ndata$ noisy temperature observations $(\mT^{(i)}_\meas, \partition{\mT})$ of $\ndata$ dies and the spatial locations of these dies $\vr = (\r_i) \in \domain^\ndata$ in the domain $\domain$ representing the surface of the wafer.
The test power profile is assumed to be fine-grained whereas temperature profiles are presumably coarse; for simplicity, the latter are also assumed to have the same time partition $\partition{\mT}$.
Since the data set is assumed to be incomplete, the number of measured dies $\ndata$ is smaller than the total number of dies on the wafer denoted by $\nchips$.
