Process variation constitutes one of the major concerns of electronic system designs \cite{chandrakasan2001, srivastava2010}. A crucial implication of process variation is that it renders the key parameters of a technological process, \eg, the effective channel length and gate oxide thickness, as uncertain quantities.
Therefore, the same workload applied to two ``identical'' dies can lead to two different power and, thus, temperature profiles since the dissipation of power and heat essentially depends on the aforementioned quantities.
Consequently, process variation leads to performance degradation in the best case and to severe faults or burnt silicon in the worst scenario.
Under these circumstances, uncertainty quantification has evolved into an indispensable asset of the fabrication workflows in order to provide them with guaranties on the efficiency and robustness of products.

An important target of uncertainty quantification is the characterization of the on-wafer distribution of a quantity of interest, deteriorated by process variation, based on measurements.
The problem belongs to the class of inverse problems since the analyzed parameter can be seen as an input to the system and the measured data as the corresponding output.
Such an inverse problem is addressed in this work: our goal is to characterize arbitrary process parameters with high accuracy and at low costs.
The goal is accomplished by tracking supplementary quantities, which are more convenient and less expensive to be measured, and employing Bayesian statistics \cite{gelman2004} to infer the needed parameters from the observed data.

Bayesian inference is utilized in \cite{zhang2010} to identify the optimal set of locations on a wafer, in which the parameter under consideration should be measured in order to characterize it with the maximal accuracy.
The expectation-maximization algorithm is considered in \cite{reda2009} in order to estimate missing test measurements.
In \cite{paek2012}, the authors consider an inverse problem focused on the inference of the power dissipation based on transient temperature maps using Markov random fields.
Another temperature-based characterization of power is developed in \cite{mesa-martinez2007} wherein a genetic algorithm is employed for the reconstruction of the power model.
It should be noted that the procedures in \cite{zhang2010, reda2009} operate on direct measurements meaning that the output is the same quantity as the one being measured.
The problem is that such direct measurements have undesirable implications as we shall discuss in \sref{motivation}.
In particular, \cite{zhang2010, reda2009} rely heavily on the availability of adequate test structures on the dies and are practical only for the secondary quantities affected by process variation, such as delays and currents, but not for the primary ones, such as various geometrical properties.
Consequently, \cite{zhang2010, reda2009} often lead to excessive costs and have a limited range of application.
The approaches in \cite{paek2012} and \cite{mesa-martinez2007}, on the other hand, concentrating on the power dissipation of a single die, are not concerned with process variation.

Our work makes the following main contribution.
We propose a novel approach to the quantification of process variation based on indirect, incomplete, and noisy measurements.
Indirectness is the key ingredient as it allows for significant savings of the costs associated with the data-harvesting stage.
Moreover, we develop and implement a solid framework around the proposed idea and perform a thorough study of various aspects of our technique.
