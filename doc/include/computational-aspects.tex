In this subsection, we discuss several aspects of our framework that help to speed up the inference procedure.

\subsubsection{Analytical Solution} \slabel{analytical-solution}
First of all, we address the complexity of $\model$. The model is expensive as it involves a system of nonlinear differential equations (see \eref{heat-de}), which should be solved numerically using, \eg, Runge-Kutta methods \cite{press2007}. In order to mitigate these repetitive computations, we utilize the approach that is discuss in \cite{ukhov2012}.
The idea is that, if the power term on the right-hand side of \eref{heat-de} stays constant, the system becomes linear and obtains an analytical solution. When the simulated time interval is short enough, the technique was found to have a negligibly small influence on the resulting accuracy; however, the speedup was found to be significant.
Thus, since $\profilePdyn$ is fine-grained, we can assume that the total power changes only at time moments $\{ \t_i \}$ in $\partition{\mP}$ (see \sref{problem-formulation}). In this way, we can stride in time solving \eref{heat-de} for each step analytically and, thus, gain a significant speedup. See \cite{ukhov2012} for further details.

\subsubsection{Proposal Distribution} \slabel{proposal-distribution}
The core of the Metropolis algorithm is the proposal distribution. This distribution has a significant impact on the efficiency of the probability space exploration. ``Efficiency'' in this case means that fewer samples, \ie, fewer forward model evaluations, are needed to draw justifiable conclusions from the data.
Therefore, before the actual sampling, we undertake an optimization procedure of the log-posterior function given by \eref{log-posterior}. The result of this optimization is a posterior mode $\hat{\vparam}$ and the corresponding observed information matrix $\mOI$ \cite{gelman2004}, which form a solid base for the proposal distribution.
A popular choice of such a distribution is a multivariate Gaussian distribution wherein the mean is the current location of the chain starting from $\hat{\vparam}$, and the covariance matrix is the inverse of $\mOI$.

\subsubsection{Sampling Strategy} \slabel{sampling-strategy}
We make use of the omnipresent parallel computing for sampling. To this end, instead of utilizing the classical proposal mentioned in \sref{proposal-distribution}---which is purely sequential as the mean for the next sample draw depends on the previous sample---we appeal to the independence sampler Metropolis algorithm \cite{gelman2004}. In this case, a typical choice of proposal is a multivariate t-distribution, which is independent of the current position of the chain:
\begin{equation} \elabel{proposal}
  \vparam \sim \studentst{\nu}{\hat{\vparam}}{\alpha^2 \mOI^{-1}}
\end{equation}
where $\hat{\vparam}$ and $\mOI$ are as in \sref{proposal-distribution}, $\nu$ is the number of degrees of freedom, and $\alpha$ is a tuning constant. Now, the posterior in \eref{log-posterior} can be computed for all samples in parallel.
