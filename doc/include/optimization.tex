In this section, we describe the objective of \stage{2}\ in \fref{algorithm}.
Although the requirements to the proposal distribution mentioned earlier are rather weak, it is often difficult to pick an efficient proposal, which would yield a good approximation with as few evaluations of the posterior in \eref{posterior} and, thus, of the data model in \sref{data-model} as possible.
This choice is especially severe for high-dimensional problems, and our problem, involving around 30 parameters as we shall see in \sref{experimental-results}, is one them.
Therefore, a careful construction of the proposal distribution is an essential component of our framework.\footnote{This has been also confirmed by our experiments. Without optimization, even for small examples, no adequate results were obtained in an affordable time. Therefore, all the experiments in \sref{experimental-results} include the optimization step.}
A common technique to construct a high-quality proposal is to perform an optimization of the posterior given by \eref{posterior}.
More specifically, we seek for such a value $\hat{\vparam}$ of $\vparam$ that maximizes \eref{posterior} and, hence, has the maximal posterior probability.
We also compute the negative of the Hessian matrix at $\hat{\vparam}$, which is called the observed information matrix and denoted by $\mOI$ (see the output of \stage{2}\ in \fref{algorithm}).
Using $\hat{\vparam}$ and $\mOI$, we can now construct such a proposal, which will allow the MH algorithm (a) to start producing samples directly from the desired regions of high probability and (b) to explore those regions more rapidly.
