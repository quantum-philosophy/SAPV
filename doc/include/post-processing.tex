At Stage~4 in \fref{algorithm}, using the set of samples $\UData$, the user computes the desired statistics about the \qoi\ such as the most probable value of the effective channel length at an arbitrary point on the wafer, the probability of an area on the wafer to be defective, \etc\ The computations boil down to the estimation of expected values with respect to the posterior distribution of $\vparam$. This estimation is done in the standard sample-based fashion, that is, in order to compute the expected value of some quantity $h$, one needs to evaluate $h$ for each $\vu_i$ in $\UData$ and then take the average value:
\[
  \E_{\vparam | \Data}(h(\u)) = \int h(\u) \f{\vparam | \Data} d\vparam \approx \frac{1}{\nsamples} \sum_{i = 1}^\nsamples h(\vu_i).
\]
It is worth emphasizing that such statistics can be estimated not only for $\u$ itself, but also for other quantities dependent on $\u$.
For example, we can revert the inference and, given an arbitrary dynamic power profile (different from the one used to collect $\Data$), reason about the corresponding power and temperature profiles, \eg, find the probability density function of the maximal values.
Also, the strength of the Bayesian approach to inference really starts to shine when one needs to take a decision of some kind based on the collected observations in $\Data$ (recall the discussion in \sref{motivation}).
