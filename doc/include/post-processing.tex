Having completed the MC sampling procedure, we obtain a collection of samples of $\u$. Since it can take time for a Markov chain to reach regions of high probability, a certain number of initial samples are typically discarded as being unrepresentative (see also \sref{proposal-distribution}); this is known as a burn-in period. Denote the shrunk data set with $\nsamples$ samples by $\UData = \{ \u_i \}_{i = 1}^\nsamples$. Now, we can compute a wide range of statistics from $\UData$, both across the wafer and across a single die: moments, probabilities, quantiles, \etc\ The computations essentially boil down to the estimation of expected values with respect to the posterior distribution of $\vparam$. This estimation is done in the standard sample-based fashion, that is, in order to compute the expected value of some quantity $h$, one needs to evaluate $h$ for each $\u_i$ in $\UData$ and then take the average:
\[
  \E_{\vparam | \Data}(h) = \int h(\u) \f{\vparam | \Data} d\vparam \approx \frac{1}{\nsamples} \sum_{i = 1}^\nsamples h(\u_i).
\]
It is worth being mentioned again that such statistics can be estimated not only for $\u$ itself, but also for other quantities dependent on $\u$. For example, we can revert the inference and, given an arbitrary dynamic power profile (different from the one used to collect $\Data$), reason about the corresponding power and temperature profiles, \eg, find the probability density function of the maximal values. Also, as discussed in \sref{motivation}, the strength of the Bayesian approach to inference starts really shine when one needs to take a decision of some kind based on the collected observations in $\Data$.
