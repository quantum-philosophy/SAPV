In this subsection, we discuss several aspects of our framework that help to speed up the inference procedure.

\subsubsection{Analytical Solution} \slabel{analytical-solution}
First of all, we need to address the complexity of the forward model $\model$. The model is expensive as it involves the system of nonlinear differential equations given in \eref{heat-de}, which, in general, should be solved numerically using, \eg, Runge-Kutta methods \cite{press2007}. In order to mitigate these repetitive computations, we utilize the approach that we discuss in detail in our earlier publication \cite{ukhov2012}. The idea is that, if the power term on the right-hand side of \eref{heat-de} was constant, the system would be linear and would have an analytical solution, which is much more efficient to work with. Therefore, we assume that the total dissipation of power stays constant between two successive moments of time of the input time partition $\partition{\mP}$ (see \sref{problem-formulation}). In this way, we can stride in time solving \eref{heat-de} for each step analytically and, thus, gain a significant speedup. See \cite{ukhov2012} for further details.

\subsubsection{Proposal Distribution} \slabel{proposal-distribution}
The core of the Metropolis algorithm is the proposal distribution. This distribution has a significant impact on the efficiency of the probability space exploration. ``Efficiency'' in this case means that fewer samples, \ie, fewer forward model evaluations, are needed to draw justifiable conclusions from the data. Therefore, before the actual sampling, we undertake an optimization procedure of the log-posterior function given by \eref{log-posterior}; in our experiments, the optimization is done using the Quasi-Newton algorithm \cite{press2007}. The result of this optimization is a posterior mode $\hat{\vparam}$ and the corresponding observed information matrix $\mOI$ \cite{gelman2004}, which form a solid base for the proposal distribution. For instance, a popular choice of the proposal distribution is a multivariate Gaussian distribution. The mean of this distribution is the current location of the chain starting from $\hat{\vparam}$, and the covariance matrix is the inverse of $\mOI$.

\subsubsection{Sampling Strategy} \slabel{sampling-strategy}
We make use of the omnipresent parallel computing. To this end, instead of utilizing the classical proposal mentioned in \sref{proposal-distribution}, which is purely sequential as the mean of the distribution for the next sample draw depends on the previous sample, we appeal to the independence sampler Metropolis algorithm \cite{gelman2004}. In this case, a typical choice of proposal is a multivariate t-distribution, which is parametrized independently of the current position of the chain:
\[
  \vparam \sim \studentst{\nu}{\hat{\vparam}}{\alpha^2 \mOI^{-1}}
\]
where $\hat{\vparam}$ and $\mOI$ are as in \sref{proposal-distribution}, $\nu$ is the number of degrees of freedom, and $\alpha$ is a tuning constant. Now, the posterior in \eref{log-posterior} can be computed for all samples in parallel.
