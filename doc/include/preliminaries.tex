\subsection{Bayesian Inference} \slabel{bayesian-inference}
Let $\vU$ be a vector of parameters that are uncertain for us either in nature or due to our limited knowledge. The goal is to characterize the distribution of $\vU$ given a data set the corresponding observations and some prior believes about $\vU$. A natural solution is to rely on Bayes' rule \cite{gelman2004}:
\begin{equation} \elabel{bayes}
  \text{Prior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}, \;\; \f{\vu | \data} = \frac{\f{\data | \vu} \f{\vu}}{\f{\data}},
\end{equation}
where $\f{\cdot}$ and $\f{\cdot | \cdot}$ denote a probability density function and a conditional probability density function, respectively. $\f{\vu}$ is known as the prior of the parameters $\vU$, $\f{\vu | \data}$ is the corresponding posterior, $\f{\data | \vu}$ is the likelihood function, and $\f{\data}$ is a normalization constant.

For certain combinations of likelihood functions and prior densities, which usually appear in relatively simple problems, it is possible to obtain closed-form expressions for the corresponding posteriors; they can even fall into one of the standard families of probability distributions. However, it is rarely the case for challenging but typically more interesting problems. For instance, when modeling a physical process, the likelihood function can easily involve a system of differential equations, which is our scenario as we shall discuss in \sref{thermal-model}. In such a situation, in order to be able to draw samples from the posterior, one usually relies on sampling techniques. In this paper, we shall utilize MCMC sampling, namely the Metropolis-Hastings (MH) algorithm \cite{gelman2004}.

\subsection{Karhunen-Lo\`{e}ve (KL) Expansion} \slabel{kl-expansion}
Let $\U: \outcomes \times \domain \to \real$ be a square-integrable stochastic process defined over a spatial domain $\domain$. Let $\fMean{\r} := \E{\U(\r)}$ and $\fCov{\r, \r'} := \E{(\U(\r) - \fMean{\r}) (\U(\r') - \fMean{\r'})}$, $\r, \r' \in \domain$, be the mean and covariance functions of $\U$, respectively. Take a vector of spatial locations $\vr = (\r_i) \in \domain^n$, and define the corresponding discretization of $\U$ as an $n$-dimensional random vector $\vU = (\U(\r_i)) \in \real^n$ endowed with the expected value $\vMean = (\fMean{\r_i}) \in \real^n$ and the covariance matrix $\mCov = (\fCov{\r_i, \r_j}) \in \real^{n \times n}$. Since any covariance matrix is a real, symmetric matrix, it admits the eigenvalue decomposition \cite{press2007} written as
\[
  \mCov = \m{V} \m{\Lambda} \m{V}^T
\]
where $\m{V}$ and $\m{\Lambda} = \diag{\lambda_i}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov$, respectively. Consequently, $\vU$ can be represented as
\[
  \vU = \vMean + \m{V} \m{\Lambda}^\frac{1}{2} \vZ
\]
where $\vZ$ is a vector of centered, normalized, and uncorrelated \rvs, which are also independent if $\U$ is Gaussian.

The decomposition above provides means of model order reduction. The intuition is that, due to the existing correlations between \rvs, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the eigenvalues $\lambda_i$ found in $\m{\Lambda}$. Assume $\lambda_i$, $\forall i$, are arranged in a non-increasing order and let $\tilde{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\tilde{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold. When this threshold is sufficiently high (close to one), the rest of the eigenvalues and their eigenvectors can be dropped as being insignificant, reducing the number of stochastic dimensions.

\subsection{Gaussian Process Regression} \slabel{gp-regression}
Let $\data = \{ (\v{x}_i, y_i) \}_{i = 1}^\nobs$ be a training set with $\nobs$ observations where $\v{x} \in \real^\nreg$ and $y \in \real$ denote the regressors and the target, respectively. Denote $\m{X} = (\v{x}_i) \in \real^{\nreg \times \nobs}$ and $\v{y} = (y_i) \in \real^\nobs$. The data are modeled as $y_i = f(\v{x}_i) + \noise_i$ where $f: \real^\nreg \to \real$ is the unknown function to be identified, and $\noise_i \in \real$ is an additive noise. We put a zero-mean Gaussian process prior on $f$:
\[
  f | \vparam \sim \gaussianp{0}{\fCov},
\]
where $\fCov{\r, \r'} = \E{f(\r) f(\r')}$ is the covariance function of $f$, dependent on a set of parameters $\vparam$, and a zero-mean Gaussian prior on the noise:
\[
  \noise_i | \sigma^2_\noise \sim \gaussian{0}{\sigma^2_\noise},
\]
where $\sigma^2_\noise$ is the variance of the noise. It can be shown \cite{rasmussen2006} that the corresponding likelihood function, posterior, and predictive distribution are Gaussian. The latter is
\begin{align*}
  & f | \m{X}_*, \data \sim \gaussian{\vMean}{\mCov}, \\
  & \vMean = \fCov{\m{X}_*, \m{X}} [ \fCov{\m{X}, \m{X}} + \sigma^2_\noise \mI ]^{-1} \v{y}, \\
  & \mCov = \fCov{\m{X}_*, \m{X}_*} - [ \fCov{\m{X}, \m{X}} + \sigma^2_\noise \mI ]^{-1} \fCov{\m{X}, \m{X}_*},
\end{align*}
where, conventionally, the covariance function $\fCov$ applied to two matrices of covariates denotes the matrix obtained by evaluating $\fCov$ on those covariates. The above expression predicts $f$ at $\nnew$ unobserved locations stored in $\m{X}_* \in \real^{\nreg \times \nnew}$ similar to $\m{X}$ defined earlier.
