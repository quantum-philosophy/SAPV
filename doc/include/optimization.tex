In this section, we describe the objective of Stage~2 in \fref{algorithm}.
The core of the Metropolis algorithm is the proposal distribution (see \sref{bayesian-inference}) as a carefully constructed proposal can significantly reduce the number of steps needed for a Markov chain to start producing representative samples; in other words, it can save a lot of evaluations of $\model$.
A common technique to construct a high-quality proposal is to perform an optimization procedure of the log-posterior given by \eref{log-posterior}. More specifically, we seek for such $\vparam$ that maximizes \eref{log-posterior}.
By doing so, we obtain a highly probable value of $\vparam$, denoted by $\hat{\vparam}$, and also compute the corresponding Hessian matrix at $\hat{\vparam}$, denoted by $\mOI$. The two form a solid base for the proposal.
For example, a classical choice is a multivariate Gaussian distribution wherein the mean is the current location of the Markov chain started at $\hat{\vparam}$, and the covariance matrix is the inverse of $\mOI$; see \cite{gelman2004, bernardo2007}.
