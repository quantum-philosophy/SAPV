Process variation constitutes one of the major concerns of electronic system designs \cite{chandrakasan2001, srivastava2010}. A crucial implication of process variation is that it renders the key parameters of a technological process, \eg, the effective channel length and gate oxide thickness, as uncertain quantities.
Therefore, the same workload applied to two ``identical'' dies can lead to two different power and, thus, temperature profiles since the dissipation of power and heat essentially depends on the aforementioned stochastic quantities.
Consequently, process variation leads to performance degradation in the best case and to severe faults or burnt silicon in the worst scenario.
Under these circumstances, uncertainty quantification has evolved into an indispensable asset of the fabrication workflows in order to provide them with guaranties on the efficiency and robustness of products.

An important target of uncertainty quantification is the characterization of the on-wafer distribution of a quantity of interest, deteriorated by process variation, based on measurements (not necessarily of the same quantity).
The problem belongs to the class of inverse problems since the analyzed parameter can be seen as an input to the system and the measured data as the corresponding output, which is opposed to direct problems wherein one studies outputs based on the knowledge of inputs (see, \eg, \cite{juan2011, juan2012}).
Such an inverse problem is addressed in this work: our goal is to characterize the key process parameters with high accuracy and at low costs.
The goal is accomplished by tracking supplementary quantities of higher levels that are more convenient and less expensive to be measured, \eg, power and temperature, and employing Bayesian statistics \cite{gelman2004} in order to infer the needed parameters from the collected observations.

Bayesian inference is utilized in \cite{zhang2010} to identify the optimal set of locations on a wafer, in which the parameter under consideration should be measured in order to characterize it with the maximal accuracy.
In \cite{paek2012}, the authors consider an inverse problem focused on the inference of the power dissipation based on transient temperature maps using Markov random fields.
Another temperature-based characterization of power is developed in \cite{mesa-martinez2007} wherein a genetic algorithm is employed for the reconstruction of the power model.
It should be noted that the inference in \cite{zhang2010} operates on direct measurements meaning that the output of \cite{zhang2010} is the same quantity as the one being measured.
This implies that the technique in \cite{zhang2010} is often impractical since direct measurements of the majority of process parameters require expensive test structures to be deployed onto each die on the wafer.
Moreover, the technique in \cite{zhang2010} is tailored for the secondary quantities affected by process variation, such as frequencies, voltages, and currents, but not the primary sources of uncertainty such as the effective channel length and gate oxide thickness.
The approaches in \cite{paek2012} and \cite{mesa-martinez2007}, on the other hand, concentrating on the power dissipation of a single die, are not concerned with process variation.

Our work makes the following main contributions.
First, we propose a novel approach to the quantification of process variation based on indirect, incomplete, and noisy measurements.
Indirectness is the key ingredient as it allows for significant savings of the costs associated with the data-harvesting stage and subsequent analysis.
Second, we develop and implement a solid framework around the proposed idea and perform a thorough study of various aspects of the framework, which are of high practical interests.
