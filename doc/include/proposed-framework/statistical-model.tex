Since, at each point of the continuum of spatial locations on the wafer, $\U$ can potentially take a different value, $\U$ is infinite-dimensional. We model $\U$ as a stochastic process $\U: \outcomes \times \domain \to \real$ in $\H$ defined over a spacial domain $\domain$ corresponding to the wafer. Uncertainties due to process variation are known to be well approximated using Gaussian distributions \cite{srivastava2010}; therefore, $\U$ is assumed to be a Gaussian process \cite{rasmussen2006}:
\[
  \U | \fMean, \fCov \sim \gaussian{\fMean}{\fCov}
\]
where $\fMean{\r} := \E{\U(\r)}$ and $\fCov{\r, \r'}$, $\forall \r, \r' \in \domain$, are the mean and covariance functions of $\U$. For simplicity, let $\fMean{\r} = \fMean$, $\forall \r \in \domain$, meaning that the function is a constant equal to some nominal value $\fMean$ of $\U$. Due to the particularities of the manufacturing process, such uncertainties often bear radial structures \cite{cheng2011}; thus, we assume that $\r$ represents the Euclidean distance from the center of the wafer to the center of the die, and the covariance function of $\U$ is exponential \cite{maitre2010}:
\begin{equation} \elabel{covariance-function}
  \fCov{\r, \r'} = \sigma_\U^2 \exp\left(-\frac{\abs{\r - \r'}}{\ell}\right)
\end{equation}
where $\sigma_\U^2$ represents the variance of $\U$, and $\ell > 0$ is known as the correlation length; the later is assumed to be given while the former is a subject of our inference.

\subsubsection{Model reduction}
The infinite-dimensional object $\U$ is reduced to the finite-dimensional case using the KL expansion introduced in \sref{kl-expansion}:
\begin{equation} \elabel{kl-approximation}
  \U(\r) \approx \klapprox{\U}{\vZ, \r} := \fMean + \sum_{i = 1}^\nvars \sqrt{\lambda_i} \: \klb_i(\r) \: \Z_i
\end{equation}
where $\{ (\lambda_i, \klb_i) \}$ are the first $\nvars$ eigenpairs of $\fCov{\cdot, \cdot}$, and $\Z_i$ are standard Gaussians. It can be seen that the stochastic process $\U$ has been replaced with an $\nvars$-dimensional vector of \rvs\ $\vZ = (\Z_i) \in \real^\nvars$.

\subsubsection{Additive noise}
Denote the forward model developed in \sref{power-model} and \sref{thermal-model} by
\begin{equation} \elabel{model}
  \mT(\r) = \model{\u, \r}
\end{equation}
which, for an outcome $\u: \domain \to \real$ of the process $\U$, transforms the test dynamic power profile $\profilePdyn$ into the corresponding temperature profile $\profileT$ computed at location $\r \in \domain$. Note, $\model$ also performs thinning of the output temperature to match the partition $\partition{\mT}$ of $\data$. Due to the imperfection of the modeling and measurement processes, a temperature profile $\mT^{(i)}_\meas$ from the data set $\data$ is assumed to deviate from the model prediction in \eref{model}. To account for this, $\mT^{(i)}_\meas$ is modeled as
\[
  \mT^{(i)}_\meas = \mT(\r_i) + \mnoise^{(i)} = \model{\u, \r_i} + \mnoise^{(i)}
\]
where $\mnoise^{(i)} = (\noise^{(i)}_{jk}) \in \real^{\ncores \times \nsteps}$ represents a matrix of an additive noise. The noise is typically assumed to be a white Gaussian noise and to be independent from $\U$ \cite{marzouk2007, el-moselhy2012}. Therefore, $\noise^{(i)}_{jk}$ is modeled as
\[
  \noise^{(i)}_{jk} | \sigma^2_\noise \sim \gaussian{0}{\sigma^2_\noise}
\]
where $\sigma^2_\noise$ is a parameter defining the variance of the noise; imposing no loss of generality, the noise is assumed to have the same magnitude for all measurements.

\subsubsection{The likelihood function}
Denote all the uncertain parameters present in the current model by
\[
  \Param = \left\{ \Z_1, \dotsc, \Z_\nvars, \sigma^2_\U, \sigma^2_\noise \right\}
\]
and an outcome of $\Param$ by $\param$. The noise yields the following likelihood function $\f{\data | \param}$ of the data $\data$, which, intuitively speaking, gives the probability of observing $\data$:
\begin{align}
  \f{\data | \param} & = \prod_{i = 1}^{\ndata} \f{\mT^{(i)}_\meas| \param, \r_i}  \elabel{likelihood} \\
  & \propto \frac{1}{\sigma_\noise^\ndcs} \exp\left( -\frac{\sum_{i = 1}^\ndata \fnorm{\mnoise^{(i)}}^2}{2 \sigma^2_\noise} \right) \nonumber
\end{align}
where $\ndcs = \ndata \ncores \nsteps$, $\fnorm{\cdot}$ is the Frobenius norm, and
\[
  \mnoise^{(i)} = \mT^{(i)}_{\meas} - \mT(\r_i) = \mT^{(i)}_{\meas} - \model{\param, \r_i}.
\]
Note, $\model{\param, \cdot}$ now denotes the forward model.

\subsubsection{The prior}
The parameters in $\Param$ are assumed to have the following priors \cite{marzouk2009}:
\begin{align}
  & \Z_i | \sigma^2_\U \sim \gaussian{0}{\sigma^2_\U}, \elabel{z-prior} \\
  & \sigma^2_\U \sim \sichisquared{\nu_\U}{\tau^2_\U}, \elabel{sigma2-u-prior}\\
  & \sigma^2_\noise \sim \sichisquared{\nu_\noise}{\tau^2_\noise}. \elabel{sigma2-noise-prior}
\end{align}
The prior for $\Z_i$ is due to the definition of a KL expansion for Gaussian processes (see \sref{kl-expansion}) and due to the multiplier $\sigma^2_\U$ of the covariance function. The last two priors, the scaled inverse chi-squared distributions, are a common, in Bayesian settings, choice for the variance of a Gaussian model \cite{gelman2004}. The parameters $\nu_\U$ and $\nu_\noise$, representing the degree of beliefs, and $\tau^2_\U$ and $\tau^2_\U$, representing the presumable values of $\sigma^2_\U$ and $\sigma^2_\noise$, should be set according to the prior knowledge. In the absence of such knowledge, a non-informative prior can be chosen for $\sigma^2_\U$ and $\sigma^2_\noise$. Taking the product of \eref{z-prior}--\eref{sigma2-noise-prior},
\begin{align}
  & \f{\param} = \prod_{i = 1}^\nvars \f{\Z_i | \sigma^2_\U} \; \f{\sigma^2_\U} \; \f{\sigma^2_\noise} \elabel{prior} \\
  & \propto \frac{1}{\sigma_\U^{\nvars + \nu_\U + 2} \sigma_\noise^{\nu_\noise + 2}} \exp\left( -\frac{\vZ^T\vZ + \nu_\U \tau_\U^2}{2 \sigma^2_\U} - \frac{\nu_\noise \tau_\noise^2}{2 \sigma^2_\noise} \right). \nonumber
\end{align}

\subsubsection{The posterior}
Since we intend to use the Metropolis-Hasting algorithm for sampling, it is sufficient to find the posterior distribution up to a constant, with respect to $\Param$. In particular, this renders the normalization constant in \eref{bayes} irrelevant. Taking the product of \eref{likelihood} and \eref{prior} and transferring it into the logarithmic space, the posterior is
\begin{align*}
  & c_1 \ln \f{\Param | \data} + c_2 = -\frac{\sum_{i = 1}^\ndata \fnorm{\mnoise^{(i)}}^2}{\sigma^2_\noise} -\frac{\vZ^T\vZ + \nu_\U \tau_\U^2}{\sigma^2_\U} \\
  & {} - \frac{\nu_\noise \tau_\noise^2}{\sigma^2_\noise} -(\ndcs + \nu_\noise + 2) \log \sigma^2_\noise - (\nvars + \nu_\U + 2) \log \sigma^2_\U
\end{align*}
where $c_1 > 0$ and $c_2$ are some constants. It is worth being noted that the expression above poses a significant computational challenge as it requires $\ndata$ evaluations of $\model$ for each sample of $\Param$.
