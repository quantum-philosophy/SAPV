\input{include/figures/wafer-qoi.tex}
Let us consider an important application of the proposed technique: the identification of the distribution of the effective channel length across the wafer. The effective channel length, hereafter denoted by $\u$, has a strong influence on the leakage power and temperature; at the same time, $\u$ is well-known to be severely deteriorated by process variation \cite{srivastava2010, juan2011, juan2012}.
Assume the technological process imposes a lower bound $\u_*$ on $\u$. This bound separates defective dies ($\u < \u_*$) from those that function properly ($\u \geq \u_*$).
In order to reduce costs, the manufacturer is interested in detecting the faulty dies and taking them out of the production process at early stages. Then, the possible actions that they might take with respect to a single die on the wafer are: (a) keep the die if it closely conforms to the specification; (b) recycle the die, otherwise.
Let the distribution of the effective channel length across the wafer be the one depicted on the left side of \fref{wafer-qoi} where the gradient from navy to dark red represents the transition of $\u$ from low to high values.\footnote{The experimental setup is described in detail in \sref{experimental-results}.}
A common approach to find this distribution is to deploy adequate test structures on the dies and measure $\u$ directly; then, an appropriate decision can be taken using the collected information. The problem in this scenario, however, is that the described procedure is technologically complex and, thus, might significantly increase the production costs.

The technique developed in this paper works differently. We apply a fixed workload to a small number of dies on the wafer and measure the corresponding temperature profiles. These profiles can be potentially corrupted by the measurement noise. Since temperature is cheap to track using, \eg, infrared cameras and no on-die test structures are required, our approach can considerably decrease the costs associated with the characterization of process variation and further decision making.
The result of our framework applied to a data set, wherein only 20 out of 316 dies on the wafer have been measured, is shown on the right side of \fref{wafer-qoi}. It can be seen that the two distributions closely match each other. Hence, based on less than $7\%$ of the dies, we can estimate the distribution of the effective channel length with high accuracy.

The proposed framework can readily be utilized to estimate probabilities of various events, \eg, $\probabilityMeasure(\u < \u_*)$. This is important since, in reality, we do not know the true values and, therefore, can reason about our decisions only in terms of probabilities. We can then reformulate the decision rule defined earlier as follows: (a) keep the die if $\probabilityMeasure(\u \geq \u_*)$ is larger than a certain threshold; (b) recycle the die, otherwise.
An illustration of this rule is given in \fref{wafer-defect} where the lower bound $\u_*$ is set to two standard deviations below the mean value of the effective channel length; the probability threshold of the action (a) is set to 0.9; the crosses mark both the true and inferred defective dies (they coincide); and the gradient from light gray to red corresponds to the inferred probability of a die to be defective. It can be seen that the inference accurately detects faulty regions.
\input{include/figures/wafer-defect.tex}

In addition, we can introduce a trade-off action: (c) expose the die to a thorough inspection (\eg, via a test structure) if $\probabilityMeasure(\u \geq \u_*)$ is smaller than the threshold of (a) and is larger than some other threshold. In this case, we can reduce costs by examining only those dies for which there is neither sufficiently strong evidence of their satisfactory nor unsatisfactory condition; for example, the dies for which $0.1 < \probabilityMeasure(\u \geq \u_*) < 0.9$.
Furthermore, one can introduce in the inference a so-called utility function, which, for each combination of an outcome of $\u$ and a taken action, returns the corresponding gain that the decision maker obtains.
The optimal decision is given by the action that maximizes the expected utility with respect to both the observed data and the prior knowledge on $\u$ \cite{bernardo2007}. In other words, all possible outcomes of the effective channel length weighted by their probabilities will be taken into account in our final decision.
