\input{include/tables/performance.tex}
In this section, we assess our framework using the inference of the effective channel length $\u$ based on temperature $\q$.
This choice f or illustration is dictated by the fact that such a high-level parameter as temperature constitutes a challenging task for the inference of such a low-level parameter as the effective channel length, which implies a strong assessment of the proposed technique.
On the other hand, the effective channel length is an important target \perse\ as it is strongly affected by process variation and considerably impacts the power/heat dissipation \cite{chandrakasan2001, srivastava2010, juan2012}; in particular, it also influences other process-related characteristics such as the threshold voltage.
The performance of our approach is expected to only increase when the auxiliary parameter $\q$ resides ``closer'' to the target parameter $\u$ with respect to the transformation $\q = \oBB{\u}$.
For instance, such a ``closer'' quantity $\q$ can be the leakage current, which, however, might not always be the most preferable parameter to measure.

Now we shall describe the default configuration of our setup, which will be later adjusted according to the purpose of each particular experiment.
We consider a 45-nanometer technological process.
The diameter of the wafer is 20 dies, and the total number of dies $\ndies$ is 316.
The number of measured dies $\nrdies$ is 20, and these dies are chosen by an algorithm, which pursues an even coverage of the wafer.
The number of processing elements in each die is four, and they are the points of taking measurements, \ie, $\nprocs = 4$.
The floorplans of the multiprocessor platforms are constructed in such a way that the processing elements form regular grids.
The dynamic power profiles involved in the experiments are based on simulations of randomly generated task graphs.
The sampling interval of these profiles is 1~ms.
The leakage model, parametrized by temperature and the effective channel length, is constructed by fitting to SPICE simulations of reference electrical circuits.
The temperature calculations are undertaken using the approach described in \cite{ukhov2012}, based on HotSpot v5.02 \cite{hotspot}.\footnote{The floorplans of the platforms, task graphs of the applications, thermal configuration of HotSpot, \etc\ are available online at \cite{sources}.}
The input data set $\UData$ is obtained as follows: (a) draw a sample of $\u$ from a Gaussian distribution with mean 45~nm and the covariance function in \eref{covariance-function} wherein the standard deviation is 5\% \cite{juan2012}; (b) perform one fine-grained temperature simulation per each of the $\nrdies$ selected dies under the corresponding dynamic power profile; (c) shrink the temperature profiles to keep only $\nsteps$, which is equal to 20 by default, evenly spaced moments of time; and (d) perturb the obtained data set using a white Gaussian noise with the standard deviation of $1~\text{K}$ (Kelvin).

Let us turn to the statistical model in \sref{statistical-model} and summarize the intuition and our assignment for each parameter of this model.
In the covariance function given by \eref{covariance-function}, the weight parameter $\eta$ and the two length-scale parameters $\ell_\SE$ and $\ell_\OU$ should be set according to the correlation patterns typical for the production process at hand \cite{chandrakasan2001, cheng2011}; we set $\eta$ to 0.7 and $\ell_\SE$ and $\ell_\OU$ to half the radius of the wafer.
The threshold parameter of the model order reduction procedure described in \sref{model-order-reduction} and utilized in \eref{kl-approximation} should be set high enough to preserve a sufficiently large portion of the variance of the data and, thus, to keep the corresponding results accurate; we set it to 0.99 preserving 99\% of this variance. The resulting dimensionality $\nvars$ of $\vz$ in \eref{kl-approximation} was found to be 27--28.
The parameters $\mu_0$ and $\tau_\u$ of the priors in \eref{priors} are specific to the considered technological process; we set $\mu_0$ to $45~\text{nm}$ and $\tau_\u$ to 5\% of $\mu_0$ \cite{juan2012}.
The parameters $\sigma_0$ and $\nu_\u$ determine the precision of the information on $\mu_0$ and $\tau_\u$ and are set according to the beliefs of the user; we set $\sigma_0$ to 1\% of $\mu_0$ and $\nu_\u$ to ten.
The latter can be thought of as the number of imaginary observations that the choice of $\tau_\u$ is based on.
The parameter $\tau_\noise$ represents the precision (deviation) of the equipments utilized to collect the data set $\QData$ and can be found in the technical specification of these equipments; we set $\tau_\noise$ to $1~\text{K}$. The parameter $\nu_\noise$ has the same interpretation as $\nu_\u$; we set it to ten as well.
In \eref{proposal}, $\nu$ and $\alpha$ are tuning parameters, which can be configured based on experiments; we set $\nu$ to eight and $\alpha$ to 0.5.
The number of sample draws is another tuning parameter, which we set to $10^4$; the first half of these samples is ascribed to the burn-in period leaving $5 \cdot 10^3$ effective samples $\nsamples$.
For the optimization in \sref{optimization}, we use the Quasi-Newton algorithm \cite{press2007}.
For parallel computations, we utilize four processors.
All the experiments are conducted on a GNU/Linux machine with Intel Core i7 2.66~GHz and 8~GB of RAM.

To ensure that the experimental setup is adequate, we first perform a detailed inspection of the results obtained for one particular example with the default configuration.
The true and inferred distributions of the \qoi\ are shown in \fref{wafer-qoi} where the normalized root-mean-square error (NRMSE) is below 2.8\%, and the absolute error is bounded by 1.4~nm, which suggests that the framework produces a close match to the true value of the \qoi.
We have also looked at the behavior of the constructed Markov chains and the quality of the proposal distribution; however, due to the shortage of space, these results are not presented here.
All the observations suggest that the optimization and sampling procedures are properly configured.

Next we use the assessed configuration and alter only one parameter at a time: the number of measured sites/dies $\nrdies$; the number of processing elements/measured points $\nprocs$ on a site; the amount of data per measurement point $\nsteps$; and the noise deviation $\sigma_\noise$.

\subsection{Number of Measured Sites}
Let us change the number of dies $\nrdies$ that have been measured.
The considered scenarios are 1, 10, 20, 40, 80, and 160 measured dies, respectively.
The results are reported in \tref{spatial-measurements}.
In this and the following tables, we report the optimization (\stage{2}\ in \fref{algorithm}) and sampling (\stage{3}\ in \fref{algorithm}) times separately (given in minutes).
In addition, the sampling time is given for two cases: sequential and parallel computing, which is followed by the total time and error (NRMSE).
The computational time of the post-processing phase (\stage{4}\ in \fref{algorithm}) is not given as it is negligibly small.
The sequential sampling time is the most representative indicator of the computational complexity scaling as the number of samples is always fixed, and there is no parallelization; thus, we shall watch this value in most of the discussions below (highlighted in bold).

We see in \tref{spatial-measurements} that the more data the proposed framework needs to process, the longer the execution times, which is reasonable.
The trend, however, is rather modest: with the doubling of $\nrdies$, all the computational times increase less than two times.
The error firmly decreases and drops below 4\% with around 20 sites measured, which is only 6.3\% of the total number of dies on the wafer.

\subsection{Number of Measured Points Per Site}
Here we consider five platforms with the number of processing elements/measurement points $\nprocs$ on each die equal to 2, 4, 8, 16, and 32, respectively.
The results are summarized in \tref{processing-elements}.
All the computational times grow with $\nprocs$.
This behavior is expected as the granularity of the utilized thermal model (see \sref{data-model} and \cite{ukhov2012}) is bound to the number of processing elements; therefore, the temperature simulations become more intensive.
Nevertheless, even for large examples, the timing is readily acceptable, taking into account the complexity of the inference procedure behind and the yielded accuracy.
An interesting observation can be made from the NRMSE: the error tends to decrease as $\nprocs$ grows.
The explanation is that, with each processing element, $\QData$ delivers more information to the inference to work with since the temperature profiles are collected for all the processing elements simultaneously.

\subsection{Amount of Data Per Measured Point}
In this subsection, we sweep the number of moments of time $\nsteps$ captured by the measured temperature profiles.
The scenarios are 1, 10, 20, 40, 80, and 160 time moments, respectively.
The results are aggregated in \tref{temporal-measurements}.
As we see, the growth of the computational time is relatively small.
One might have expected this growth due to $\nsteps$ to be the same as the one due to $\nprocs$ since, formally, the influence of $\nprocs$ and $\nsteps$ on the dimensionality of $\QData$ is identical (recall $\vq^\meas \in \real^{\nrdies \nprocs \nsteps}$).
However, the meaning of the two numbers, $\nprocs$ and $\nsteps$, is completely different, and, therefore, the way they manifest themselves in the algorithm is also different.
Therefore, the corresponding amounts of extra data are being treated differently leading to the discordant timing shown in \tref{processing-elements} and \tref{temporal-measurements}.
The NRMSE in \tref{temporal-measurements} has a decreasing trend; however, this trend is less steady than the ones discovered before. The finding can be explained as follows.
The distribution of the time moments in $\QData$ changes since these moments are kept evenly spaced across the corresponding time spans of the input power profiles.
Some moments of time can be more informative than the other.
Hence, more or less representative samples can end up in $\QData$ helping or misleading the inference.
We can also conclude that a larger number of spatial measurements is more advantageous than a larger number of temporal measurements.

\subsection{Deviation of the Measurement Noise}
Next we vary the standard deviation of the noise (in Kelvins), affecting the data $\QData$, within the set $\{ 0, 0.5, 1, 2 \}$ coherent with the literature \cite{mesa-martinez2007}. Note that the corresponding prior distribution in \eref{priors} is kept unchanged. The results are given in \tref{noise-deviation}.
The sampling time is approximately constant. However, we observe an increase of the optimization time with the decrease of the noise level, which can be ascribed to wider possibilities of perfection for the optimization procedure.
A more important observation, revealed by this experiment, is that, in spite of the fact that the inference operates on indirect and drastically incomplete data, a thoroughly calibrated equipment can considerably improve the quality of predictions.
However, even with a high level of noise of two degrees---meaning that measurements are dispersed over a wide band of 8~K with a large probability of more than 0.95---the NRMSE is still only 4\%.

\subsection{Sequential vs. Parallel Sampling}
Let us summarize the results of the sequential and parallel sampling strategies.
In the sequential MH algorithm, the optimization time is typically smaller than the time needed for drawing posterior samples.
The situation changes when parallel computing is utilized. With four parallel processors, the sampling time decreases 3.81 times on average, which indicates good parallelization properties of the chosen sampling strategy.
The overall speedup ranges from 1.49 to 2.75 with the average value of 1.77 times, which can be pushed even further employing more parallel processors.
