Using the set of samples $\UData$, we can compute a wide range of statistics, both across the wafer and across a single die: moments, probabilities, quantiles, \etc\ The computations boil down to the estimation of expected values with respect to the posterior distribution of $\vparam$. This estimation is done in the standard sample-based fashion, that is, in order to compute the expected value of some quantity $h$, one needs to evaluate $h$ for each $\u_i$ in $\UData$ and then take the average value:
\[
  \E_{\vparam | \Data}(h(\u)) = \int h(\u) \f{\vparam | \Data} d\vparam \approx \frac{1}{\nsamples} \sum_{i = 1}^\nsamples h(\u_i).
\]
It is worth being underlined that such statistics can be estimated not only for $\u$ itself, but also for other quantities dependent on $\u$. For example, we can revert the inference and, given an arbitrary dynamic power profile (different from the one used to collect $\Data$), reason about the corresponding power and temperature profiles, \eg, find the probability density function of the maximal values. Also, the strength of the Bayesian approach to inference really starts to shine when one needs to take a decision of some kind based on the collected observations in $\Data$; recall the example in \sref{motivation}.
