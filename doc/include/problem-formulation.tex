Let $\probabilitySpace$ be a probability space where $\outcomes$ is a set of outcomes, $\sigmaAlgebra$ is a $\sigma$-algebra on $\outcomes$, and $\probabilityMeasure: \sigmaAlgebra \to [0, 1]$ is a probability measure \cite{durrett2010}. A random variable (\rv) is defined as a $\sigmaAlgebra$-measurable function $X: \outcomes \to \real$. An $n$-dimensional \rv\ is then a vector of $n$ \rvs. In what follows, the probability space $\probabilitySpace$ is always implied.

Consider a heterogeneous multiprocessor system that consists of $\nprocs$ processing elements and is equipped with a thermal package. The processing elements are the active components of the system identified at the intended level of granularity (processors, ALUs, caches, registers, \etc). Let $\specification$ be a thermal specification of the system defined as a collection of temperature-related information: (a) the floorplans of the active layers of the chip; (b) the geometry of the thermal package; (c) the thermal parameters of the materials that the chip and package are made of. In addition, the system depends on an uncertain parametrization $\u$, which manifests itself in deviations of the actual power dissipation from nominal values and, consequently, in deviations of temperature from the one corresponding to the nominal power consumption.

A power profile $\profileP$ is a tuple composed of a data matrix $\mP = (\vP_i) \in \real^{\nprocs \times \npsteps}$, $\vP_i \in \real^\nprocs$, that captures the power dissipation of all the $\nprocs$ processing elements at $\npsteps$ moments of time and a (column) vector $\partition{\mP} = (\t_i) \in \real^{\npsteps}$ with positive and strictly increasing components that specifies these moments of time. The definition of a temperature profile $\profileT$ is the same as the one for power except that the data matrix $\mT = (\vT_i) \in \real^{\nprocs \times \nsteps}$, $\vT_i \in \real^\nprocs$, contains temperature.

The goal of this work is to develop an UQ framework for characterization of the uncertain parametrization $\u$, impacting the platform, that is cheap from both financial and computational perspectives. To this end, we propose the use of indirect measurements: instead of measuring $\u$, which is typically expensive to undertake, we aim at inferring $\u$ from temperature measurements. More precisely, apart from the thermal specification $\specification$ of the platform, the input to our technique is a data set that contains temperature profiles, corresponding to an arbitrary dynamic power profile, collected at several spatial locations on the wafer. Denote by
\[
  \Data = \left( \profilePdyn, \; \; \{ (\mT^{(i)}_\meas, \partition{\mT}) \}_{i = 1}^\ndata, \; \; ( \r_i )_{i = 1}^\ndata \right)
\]
such a data set composed of (a) a dynamic power profile $\profilePdyn$ of the platform; (b) $\ndata$ temperature measurements $(\mT^{(i)}_\meas, \partition{\mT})$, corresponding to $\profilePdyn$, of $\ndata$ dies on the wafer; (c) the locations $\vr = (\r_i) \in \domain^\ndata$ of these dies in the spatial domain $\domain$ representing the wafer. The test power profile is assumed to be fine-grained whereas temperature profiles are presumably sparse; for simplicity, the latter are also assumed to have the same time partition $\partition{\mT}$. We require the framework that we are to develop in this paper to operate not only on indirect measurements, but also to take into account the fact that (a) the data are incomplete, \ie, the number of measured dies $\ndata$ is much smaller than the total number of dies on the wafer $\nchips$, and (b) the data contain the measurement noise.
