Having completed the sampling procedure, we obtain a collection of samples of $\u$. Since it can take time for a Markov chain (see \aref{bayesian-inference}) to reach regions of high probability, a certain number of initial samples are typically discarded as being unrepresentative (see also \sref{proposal-distribution}); this is known as a burn-in period. Denote such a shrunk data set with $\nsamples$ samples by $\UData = \{ \u_i \}_{i = 1}^\nsamples$. Now, we can compute a wide range of statistics from $\UData$, both across the wafer and across a single die: moments, probabilities, quantiles, \etc\ The computations boil down to the estimation of expected values with respect to the posterior distribution of $\vparam$. This estimation is done in the standard sample-based fashion, that is, in order to compute the expected value of some quantity $h$, one needs to evaluate $h$ for each $\u_i$ in $\UData$ and then take the average value:
\[
  \E_{\vparam | \Data}(h(\u)) = \int h(\u) \f{\vparam | \Data} d\vparam \approx \frac{1}{\nsamples} \sum_{i = 1}^\nsamples h(\u_i).
\]
It is worth being underlined that such statistics can be estimated not only for $\u$ itself, but also for other quantities dependent on $\u$. For example, we can revert the inference and, given an arbitrary dynamic power profile (different from the one used to collect $\Data$), reason about the corresponding power and temperature profiles, \eg, find the probability density function of the maximal values. Also, the strength of the Bayesian approach to inference starts really shine when one needs to take a decision of some kind based on the collected observations in $\Data$; recall the example in \sref{motivation}.
